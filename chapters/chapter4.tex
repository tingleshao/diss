\chapter{AR}
\section{Introduction}
\label{sec:intro}

In this modern age of the Internet of Things (IoT), it is now possible to literally glue tiny computers to everyday objects, so that they can sense, react, and tell their own stories. The IoT community has embraced wireless standards such as Bluetooth Low Energy (BLE) and developed programmable `beacon' devices that periodically broadcast a small amount of preloaded data, while lasting for multiple years on a coin-cell battery. Broadcast messages from beacon devices typically contain information about an object, a location, a web-resource, or just an arbitrary string. This connectionless mode of BLE does not require a receiver to pair/bond or connect to a sender, and hence, there is no overhead of connection setup and no inconvenience of requiring a user to enter pins and passwords. These broadcast messages are received by a BLE capable mobile device to obtain relevant information just-in-time and on-the-spot.  Emerging applications of beacon devices include advertising merchandise in retail stores, identifying late passengers at the airports, authorizing people at the hospitals, smarter signage, indoor navigation, and tracking moving platforms like airline cargo containers, computers on wheels, museum artworks, or even humans.


The enabling technology behind these applications is the ability of a beacon to simply broadcast a few bytes of data (called UUID) as BLE 4.0 advertisement packets at a rate of less than 16 bytes/sec. The bound in data rate comes from the lifetime requirement of these devices. Such a tight budget on payload size and the maximum data rate have limited a beacon's capability to only be able to broadcast an identifier or a small amount of text (effectively $\sim$18 bytes). The next generation BLE 5.0 beacon is expected to have an 8X increase in broadcasting capacity ($\sim$256 bytes). Such an increase opens up the possibility to design beacons that can serve larger assets. We are particularly interested to know --- \emph{whether BLE beacons are capable of storing and broadcasting data structures describing 3D objects, so that nearby mobile devices are able to receive and render those virtual objects onto the real-world, and to have a seamless mobile augmented reality experience}. It turns out that the answer is --- \emph{yes}, but in designing such a system, we need to deal with at least two fundamental challenges. Before we dig into their details, let us look at the benefits of a beacon-based mobile augmented reality system.

There are several advantages of having an augmented reality system that consists primarily of a set of BLE beacons. First, the system will be \textbf{low cost.} Compared to today's \$3,000 augmented reality headsets, a beacon-based infrastructure will be several orders of magnitude cheaper. Second, beacons would store 3D objects locally and broadcast them over connection-less advertising channels \textbf{without requiring any Internet connectivity}. This makes the system simpler and in many cases hassle-free as cellular signals indoors is often unusable and connecting to free WiFi often requires accepting too many agreement pop-ups and occasionally watching an advertisement video. Third, beacons are designed to \textbf{last for a long time} with battery power, which makes these systems easier to setup and maintain when compared to wall power or frequent battery replacements. Fourth, in many modern buildings, \textbf{beacons are already in place}. Setting up a mobile augment reality system in those buildings would practically cost nothing.


%, e.g., simple html pages or thumbnail images.
%e.g., an image, carried by connectionless BLE advertisement packets. However, even a simple $72\times 72$ PNG image, such as the Android launcher icon, has a size of over 3KB. To store and broadcast this image, either we require to use a dozen of BLE 5.0 beacons, or we will have to accept a very long image transmission and loading time.

%Image compression is a natural way to deal with this problem. Existing image compression algorithms, however, fail to achieve the desired compression ratio for an image to be broadcasted over BLE. Hence, a fundamental challenge toward realizing an image beacon is to devise an algorithm that efficiently represents an image using as few bits as possible, while taking into account the application-driven limits on the number of usable beacons per image, broadcast message size, data rate, latency, and lifetime. In an earlier work~\cite{shaoyears}, we devised an image beacon system that broadcasts binary images of a few limited categories (e.g., handwritten characters) only. This paper is a continuation to that line of work, but this time, we have taken a harder challenge, i.e., to develop a beacon system that works for color images, e.g., images taken with a mobile phone.

The main challenges in any augmented reality systems are: 1) communication of 3D object data that includes visual features, textures, rendering information, and a time series of these objects in case of video, and 2) the location and pose of the viewer so that the object can be overlayed on the real-world at the correct location and orientation. Furthermore, in case of mobile augmented reality, as the user moves, the object needs to be reoriented and redrawn based on his current position. For a seamless experience, all these has to happen in real-time as well. BLE beacons, to some extents, provide support for both storage and localization. There have been ongoing investigation on BLE's capabilities to store complex data structures such as images~\cite{shaoyears}. Recently, Google started to experiment with an idea called \textit{`Fat Beacons'}, where they are looking into broadcasting html pages over BLE. Indoor localization and navigation using BLE signals~\cite{zhuang2016smartphone, martin2014ibeacon} is another active area of research. In this paper, we decide to combine these two promising aspects of BLE to enable more than what we have achieved with beacons so far.

%Emerging applications of beacon devices include advertising merchandise in retail stores~\cite{pierdicca2015low}, identifying late passengers at the airports, authorizing people at the hospitals, smarter signage, indoor navigation~\cite{martin2014ibeacon}, and tracking moving platforms like airline cargo containers, computers on wheels, museum artworks, or even humans~\cite{conte2014bluesentinel}. The enabling technology behind these applications is the ability of a beacon device to simply broadcast a few bytes of data (called UUID) as BLE advertisement packets at a rate of less than 16 bytes/sec. The bound in data rate comes from the lifetime requirement of these devices. Such tight budgets on payload and maximum data rate has limited a beacon device's capability to only be able to broadcast an identifier or a small amount of text (about 16--18 bytes). To transmit a moderate sized image, either we require to use hundreds of beacon devices, or we will have to accept a very long transmission delay.

%Hypothetically, if we could broadcast high-resolution images from a beacon device in real-time, the technology would enable even more powerful and feature rich applications. Like the web has evolved from serving hypertexts to streaming multimedia contents, we envision that the natural successor of a beacon device would be the one that broadcasts images, while meeting the same energy and lifetime requirement. Applications of such an image beacon system would be in scenarios where there is no Internet connectivity but there is a need for storing and broadcasting information that can be best described by an image. Potential applications of beacon image systems include coordinating rescue workers in disaster areas, creating a bread-crumb system for adventurous hikers and mountaineers, remote surveillance (when coupled with a camera), or even a simple system just to let someone know that `I was here'.


%Being able to broadcast images from beacons enables more powerful and feature rich applications than the ones supported by today's beacons. We envision that like the web has evolved from serving hypertexts to streaming multimedia contents, the natural successor of today's beacon devices would be the ones that broadcast images. Applications of image beacons would be in scenarios where there is no Internet connectivity but there is a need for storing and broadcasting information that can be best described by an image. For example, coordinating rescue workers in disaster areas, creating a bread-crumb system for adventurous hikers and mountaineers, remote surveillance (when coupled with a camera), or even a simple system just to let someone know that `We were here'.  Recently, Google started to experiment with an idea called \textit{`Fat Beacons'}, where they are looking into broadcasting html pages over BLE. However, for lack of a suitable image compression technique, the pages do not support images.

%Our work will complement such efforts.

%In this paper, we chase this seemingly impossible goal of creating a beacon device that efficiently broadcasts images over a long period. As a first step toward realizing an image beacon, we explore the challenges to broadcasting binary images of different categories (e.g., alpha-numeric characters, basic shapes, and arbitrary binary images), and design algorithms to efficiently store contents of an image inside a set of beacon devices. The set of beacons simultaneously broadcasts chunks of an image over BLE, which are captured by a mobile device to reconstruct the image. A fundamental challenge toward achieving this is to efficiently represent an image using as few bits as possible. Standard image compression algorithms are not good enough to archive the required compression ratio so that an image can be stored inside a beacon. We investigate image approximation/coding techniques that take into account the limits on number of beacon devices, number of bits available in a beacon device, data rate, latency, and lifetime. Based on empirical analysis, we devise a patch-based image approximation algorithm which greatly reduces the image data while keeping the image distortion under a threshold. We investigate the tradeoffs between the image quality and the power consumption to determine the best set of parameters for the system under user-specified constraints.

%In this paper, we chase this seemingly impossible goal of creating an image beacon system that efficiently broadcasts color images, carried by BLE broadcast messages, over an extended period of time. We propose a self-contained system that stores and broadcasts actual image contents as opposed to IDs, links, or URLs of an image. We assume availability of no additional information on the broadcasted image from any other sources -- globally (on the web) or locally (on a user's smartphone that receives the broadcast).

In this paper, we chase this seemingly impossible goal of creating the first mobile augmented reality infrastructure that is based primarily on a set of low-cost BLE beacon devices. Our target application is a motion capture scenario where a user (e.g. an actor, a doctor, or a lecturer) would enter into an area being monitored and make natural gestures while a distributed camera system would capture his motions. Later these captured movements can be replayed and viewed in 3D for various types of post-facto analysis purposes such as training and skill improvement.

We propose a self-contained system that consists of a cluster of BLE beacon units that are connected to an embedded micro-controller and a low-cost stereo camera. Once the system is installed and set up, it operates in two phases. The first phase is dedicated to detecting dynamic events in the area being monitored and to capture and store sufficient information abut moving objects or subjects in the scene. This information is compressed, stored, and broadcasting while meeting the storage and expected lifetime requirement of the system. The second phase is dedicated to receiving and rendering the 3D virtual objects and placing them at the right location and at right scale as a viewer moves and looks at the scene through his smartphone.

%The crux of the system is an algorithm that analyzes an image to identify its `important' semantic regions (as defined by the user or the use case) and then encodes them differently than the rest of the image to reduce the overall image size. The image data are written to and read from the image beacon system using a smartphone application, which runs the proposed compression and rendering algorithms. We use the term `beacon system' instead of `a beacon', since a compressed image may still require more than one physical beacons to ensure its acceptable quality. Allowing multiple beacons per image makes the system flexible. It widens our scope for optimizations and helps satisfy users who are willing to dedicate more beacons for better results. Besides, until BLE 5.0 is available, we need to simulate its broadcast capacity with multiple BLE 4.0 devices anyways.

The crux of the system are two algorithms that are central to the two phases of the system. The first of which intelligently determines 1) a least number of most informative and useful visual features, and 2) a minimal amount of information about the moving parts of a 3D object, and store this extremely compressed information into the limited storage of the beacons. The second algorithm utilizes the BLE signal strength and combines it with the user's smartphone's IMU and camera images to accurate estimate his position and the orientation in real-time.


%a standard JPEG image, converts it to binary format, and shows the user a preview of the compressed image to be written. The user is allowed to change the settings (e.g., the number of available beacons and/or expected device lifetime) and the app immediately shows the best possible compressed image under these constraints.

%We have developed a prototype of an image beacon system using a set of commercially available Estimote beacons~\cite{ESTIMOTE}, and developed an Android application that takes images of an object of interest along with user-specified requirements and constraints on broadcasting the image as inputs, generates previews of the image to be written, writes the image representation into a set of beacons, and reads the broadcasted image back. Figure~\ref{fig:beacons} shows an example scenario where a user snaps photos of a gnome statue which he is interested in broacasting. The smartphone application performs image processing on the phone to produce multiple versions of broadcast image. The user selects one of these compressed images that satisfies his requirements (e.g. available beacons, image quality, lifetime, and image loading latency). The user is allowed to change his requirements and the app immediately shows options for the best possible compressed images under those constraints. The application writes the image data into the beacon system and the image is broadcasted by the beacons. A reader application reads the broadcasted image and displays it on the phone.

We have developed a prototype of the system, called the MARBLE, that consists of eight Estimote beacons~\cite{ESTIMOTE} connected to eight Raspberry Pis~\cite{RPI}, each having two Arducam~\cite{ARDUCAM} cameras attached to it. The prototype has been thoroughly tested to quantify its CPU and memory usage, as well as the accuracy of feature selection and localization algorithms. We demonstrate MARBLE by setting up an indoor motion capture scenario where 10 volunteers make five types of gestures for about three seconds while the system captures and stores their motions. Later, they enter the scene, walk around, and view the captured actions in 3D through their mobile phones.

The main contributions of this paper are as follows:


%We perform an in-depth evaluation of the beacon system. We describe a set of results showing the tradeoffs between system lifetime and image quality, when the image type and the number of beacons are varied. We also deploy an image beacon system indoors, and perform a user study in a real-world scenario in order to have a subjective measure of the quality of the received images, where a group of $20$ participants are asked to identify objects from their beaconed images of various resolutions, and locate it among a set of similar looking objects in the real-world.

%\begin{itemize}[leftmargin=10pt]
	%\vspace{-0.25em}
	%\item To the best of our knowledge, we are the first to propose a BLE beacon-based mobile augmented reality system.

	%\vspace{-0.25em}
%	\item We devise two algorithms: 1) an algorithm that determines a least number of most informative and useful visual features, and a minimal amount of information about the moving parts of a 3D object, and 2) an algorithm that utilizes the BLE signal strength and combines it with the user's smartphone's IMU and camera images to accurate estimate his position and the orientation in real-time.

%	\item We have developed and evaluated a prototype of the proposed system. Our evaluation shows that the system takes about 170ms to capture an object's motion, 613ms to render the scene, the selected features are 95\%-100\% accurate in determining the reference view, and the mean localization error is 14.5 cm.

%	\item We conduct a user study involving 10 participants and demonstrate that they system is capable of capturing free hand gestures and when replayed back, the users were able to view and experience them in real-time.

%\end{itemize}

\section{Problem Formulation}
\label{sec:problem}

\subsection{Generic Problem Setting}

The problem is formally stated as: given an image $\mathrm{x}$ (where each pixel is represented by $\mathrm{b}$ bit) having the dimensions of $\mathrm{N \times M}$, the number of available beacon devices $\mathrm{K}$, the payload size of each beacon packet $\mathrm{C}$ bytes, the maximum allowable broadcast rate of $\mathrm{R}$ packets/sec, and the maximum allowable latency for an image $\mathrm{T}$, the objective is to find an approximate representation of the image $\mathrm{\hat{x}}$ so that the lifetime $\mathrm{\tau}$ of the beacon system is maximized while the approximation ratio $\mathrm{\lambda(x, \hat{x}) \in [0,1]}$ of the image is high ($\mathrm{\lambda = 1}$ means no distortion). Now, for a single beacon, the broadcast rate:
\begin{equation}
	\mathrm{R = \bigg( \frac{bNM}{8C} \bigg) \frac{1}{T}}
	\label{eq:1}
\end{equation}

For K beacons, considering $\log K$ overhead bits for addressing the beacons, and $K$ times more payload capacity:
\begin{equation}
	\mathrm{R = \bigg( \frac{bNM + \log K}{8CK} \bigg)  \frac{1}{T}}
	\label{eq:2}
\end{equation}

Both ~(\ref{eq:1}) and ~(\ref{eq:2}) are for undistorted images.



The lifetime $\mathrm{\tau}$ of a BLE device depends on its inter packet interval and in general, $\mathrm{\tau \propto \frac{1}{R}}$. Replacing $\mathrm{R}$ and incorporating approximation ratio $\mathrm{\lambda}$ into~(\ref{eq:2}):

\begin{equation}
	\mathrm{\frac{1}{\tau} \propto \bigg( \frac{\lambda bNM + \log K}{8CK} \bigg)  \frac{1}{T}}
	\label{eq:3}
\end{equation}

The above equation relates the lifetime of an image beacon system and the approximation ratio of any image compression algorithm. Ideally, we look for an image approximation algorithm that achieves a sufficiently large $\mathrm{\lambda}$ for a reasonably high lifetime of the system.



\subsection{Broadcast Capacity of Bluetooth LE}

According to the BLE 4.0 specification, the maximum payload size $C$ available in beacons is 18 bytes. However, there are 33 reserved characters that cannot be read from the beacon devices. So, practically the payload size is $\lfloor \log (256-33)^{17} \rfloor \approx 132$ bits $\approx 16$ bytes.

The expected lifetime of BLE beacons depends on the inter-packet interval~\cite{dementyev2013power}. For example, a BLE 4.0 beacon would last up to 3.5 years, if a packet is sent at every second (i.e. R = 1). Therefore, for a beacon system to last for 3.5 years, its broadcast bandwidth cannot exceed 16 bytes/sec.

Recently, BLE 5.0 has been announced~\cite{BLE5} to offer an 8X increase in broadcast capacity and a 2X increase in transmission speed. It is scheduled to be released in early 2017. In coming days when BLE 5.0 capable devices will be widespread, we expect to have a $128$ byte sized payload and about 256 bytes/sec broadcast bandwidth.

\subsection{The Case for Loss-Less Image Broadcast}

The size of a typical $72\times 72$ PNG image can be anywhere between $3-13$ KB. Therefore, to transmit such an image, a BLE 4.0 beacon would require $191-832$ broadcast packets, or alternatively, we would require up to $K = 832$ beacons to simultaneously broadcast different slices of an image. The latency of a complete image transmission cycle would be up to $\mathrm{T = 13.9}$ minutes for a single beacon, or 1 second for a set of $832$ beacons.

When BLE 5.0 beacons will replace 4.0, the transmission latency will drop to $52$ seconds for one beacon, or 1 second 52 of them. Therefore, without compressing the image content, even the new BLE 5.0 beacons will not be able to support a fast image beacon system with a reasonably small number of beacons.

%Recall that our goal is to have an image compression routine that fit the beacon systems limited storage space.

\subsection{The Case for Compressed Image Broadcast}

If standard image compression algorithms could generate compressed images that meet the size and quality requirements of an image beacon system, the problem would have been already solved. But the fact is, even the best of existing image compression methods, such as JPEG/JPEG2000 and PNG, are not capable of optimizing for both quality and size at the same time. Figure~\ref{fig:common_codec_comparison} illustrates that JPEG/JPEG2000 generates extremely poor quality images given a size requirement of 300 bytes even for a very low-resolution ($64 \times 64$ pixels) image. On the other hand, to have a compressed image of acceptable quality (having a minimal useful visual information to the viewer), JPEG/JPEG2000 takes about 2K bytes.

% [XXX mention encoder used]. For images compressed in both codec, without having looked at the high quality version of the image, it is hard to recognize what is in the low quality version of the image.
%\begin{figure}[!htb]
%    \begin{center}
	%    \includegraphics[width=0.49\textwidth]{img/common_codec_comparison.pdf}
	%    \vspace{-2em}
	%    \caption{\footnotesize A 64x64 resolution image compressed in high/low quality settings using JPEG/JPEG2000: (a) JPEG high quality, 1963 bytes (b) JPEG2000 high quality, 2026 bytes (c) JPEG lowest possible quality, 738 bytes (d) JPEG2000 lowest possible quality, 391 bytes.}
	%    \label{fig:common_codec_comparison}
%    \end{center}
%\end{figure}

%\begin{figure}[!htb]
%    \begin{center}
%    	\vspace{-1em}
	%    \includegraphics[width=0.25\textwidth]{img/common_codec_comparison2.pdf}
	%    \vspace{-1em}
	%    \caption{\footnotesize Two types of 64x64 resolution image compressed in PNG (a) from natural scene, 12112 bytes (b) JPEG2000 high quality, 1012 bytes. PNG is good for handling images with large uniform color regions. }
	%    \label{fig:png_block}
	%    \vspace{-1em}
%    \end{center}
%\end{figure}

PNG and Vector Graphics image, on the other hand, have the potential to generate a smaller compressed image that \textit{may} fit our constraints. However, these codecs generate smaller images only if the input image is of a specific type -- such as an image containing a few regions of uniform colors like a cartoon drawing, or when the shape is not complicated. This is illustrated in Figure~\ref{fig:png_block}. In general, PNG and Vector Graphics image encoding do not meet the requirements of an image beacon system that broadcasts color images taken by a smartphone user.

%But we envision an image beacon system that would able to store and broadcast images taken from the real world using a user's smarptonhe camera. In general, PNG and Vector Graphics image encoding do not meet the requirements for broadcasting those types of images.

\section{Overview of \Sys}
\label{sec:system}


The \Sys is a mobile augmented reality system that uses a cluster of off-the-shelf, low power, storage and bandwidth constrained Bluetooth Low Energy (BLE) beacon units as an infrastructure. The system operates in two phases: 1) the capture and store phase, and 2) the read and render phase.

Figure~\ref{fig:beacons} (left) shows that a person \rednote{enters} an area being monitored with \Sys. A set of cameras capture his actions and stores them inside beacons. Later (right), his activity during the first phase is viewed through a 3D augmented reality application on a mobile phone. \rednote{Due to the low energy consumption feature of BLE protocol, the broadcasting phase is extremely energy efficient. In most application scenarios of \Sys, capture phase is only performed once, or the capture is triggered by a rare event that only happens every few hours or few days. Therefore, in most of the time, \Sys is working in the broadcasting phase and the expected system lifetime can be more than 6 years.}

\begin{figure}[!thb]
	\begin{center}
		\includegraphics[width = 0.49\textwidth]{img/fig1.png}
		\caption{The two phases of \Sys indoor augmented reality system: capture (once) and render (many times).}
		\label{fig:beacons}
	\end{center}
	\vspace{-1em}
\end{figure}

\begin{figure*}[!htb]
	\begin{center}
		\includegraphics[width = \textwidth]{img/photo2.png}
		\caption{System in Action. (a): in capture phase, a person's gesture and movement is captured. (b): in render phase, the virtual object avatar is rendered in the empty environment. (c): screenshot of the viewer's screen in render phase.}
		\label{fig:run_pi}
	\end{center}
\end{figure*}

\begin{figure*}[!htb]
	\begin{center}
		\includegraphics[width = \textwidth]{img/overview.pdf}
		\caption{A block diagram illustrating the work flow of \Sys.}
		\label{fig:overview}
	\end{center}
	\vspace{-1.5em}
\end{figure*}

\subsection{Two Phases of \Sys}

During the \emph{capture and store} phase, a set of low-power cameras are used to capture the dynamics of a scene. To simplify the design of each unit, we attach a stereo camera to each BLE beacon via a low-power embedded microcontroller. However, this is not a requirement in \Sys for every BLE beacon to have its own camera, i.e. a camera can be shared by multiple beacons. We assume that the duration of dynamic events during this phase is relatively shorter, and hence, the energy consumption due to active cameras is not excessively high. Once the microcontroller computes the minimal necessary information about the scene, and the 3D objects and their motions to store, data is sent to the beacon for broadcasting over the connectionless BLE advertisement packets.

During the \emph{read and render} phase, a mobile device such as a smartphone or a augmented reality headset with  a BLE capability, receives the broadcasts and renders the previously captured scene in 3D. A major task in this phase is to determine the position and orientation of the mobile device, so that the objects are rendered at the right position and orientation in the superimposed space created by the virtual and the real environment as the users walks in the space. To achieve this, the received signal strengths of the BLE beacons, in combination with the IMU and camera of the viewer's mobile \rednote{device}, is used.



%The two phases of \Sys are illustrated in Figure~\ref{fig:beacons}. For example, if a person enters the monitored area, his motion is captured and stored


%This section describes an AR indoor surveillance 3D capture and replay system. The system is a realization of our idea of AR with BLE beacons. To the best of our knowledge, we are the first to illustrate an end to end BLE beacon based system with augmented reality 3D rendering.

\subsection{Internal Modules and Basic Workflow}

During the set up of \Sys, a number of beacon units are placed at fixed positions in an indoor environment surrounding the area to be monitored. Assuming a camera is attached to each beacon unit, each beacon takes pictures of the empty space -- capturing only the environmental artifacts such as interesting points on the wall, furniture, and portions of the floor and the ceiling it sees. Each beacon then computes and stores a unique set of visual features (\emph{`oriented fast and rotated brief'} (ORB) features~\cite{rublee2011orb}) of the view it observes. This view serves as the baseline view for a beacon, and is later used during the view tracking of the mobile user.

%each beacon has a set of visual features (in our case, ORB~\cite{ref} features) stored. Every beacon contains a unique set of visual features that are extracted from an image taken by a camera placed at the location of the beacon. Features of these images are later used to match the view of a user to track his location during the read and render phase.

The workflow of our system is illustrated in Figure~\ref{fig:overview} along with the internal modules for each of the two phases of \Sys.

The capture and store phase starts with the detection of any kind of change in the scene being monitored, e.g., when a subject enters into the area or an already present object or the subject moves. This awakens the cameras attached to the beacons which starts capturing and processing image. Processing the image stream involves simple predefined tasks such as detecting predefined interesting artifacts like a human body. If an object of interest is detected, key points corresponding to its moving parts (e.g., heads and hands in case of a person) are recorded over time inside the beacon storage.

%a person enters the room where the system is deployed. The multiple shooting cameras attached to Raspberry Pi running pre-defined interesting artifacts such as human body detection programs then detect the person. The program running on Raspberry Pi locates the person in the video frame, detects the head and hands of the person, and stores the series key points describing the locations of head and hands over time into the beacons.

During the rendering phase, each beacon broadcasts its unique set of visual features and a subset of the captured 3D point time series data. At the receiving end, the viewer's smartphone receives two types of information from all beacons: 1) broadcasted visual features, and 2) received signal strengths. The smartphone combines this information with its own view as seen from its own camera to obtain a BLE-aided image-based location and pose estimate of its own. This is further combined with the smartphone's IMU data to accurately determine the position and orientation of the smartphone. Finally, location data and 3D object data are used to render the 3D object on his smartphone in real-time.

%The data is received by a viewer's smartphone in the room. The visual features, along with the beacon signal strength and the smartphone's IMU values are used in estimating the smartphone’s camera position and pose. Then based on the estimation, the 3D object that represents a moving person is rendered on the smartphone’s screen along with the camera preview.

\subsection{Advantage of \Sys}

There are several \textcolor{red}{advantages} of using \Sys compared to existing mobile augmented reality systems:

\begin{enumerate}
    \item \textit{Low Cost:} \Sys is built upon low-cost technologies. The current prototype, consisting of eight Estimote BLE beacons, two Raspberry Pis with Arducam cameras, cost about \$150 and is sufficient to monitor an indoor space of \rednote{6.5m $\times$ 5.3m}. The whole setup is 6--20 times less costly than existing augmented reality headsets~\cite{vuzix, hololens}.

    \item \textit{Internet Free:} \Sys relies only on Bluetooth connectivity. It works in indoor environments where Internet access in unavailable or an inconvenience. Being Internet-free has some advantages; e.g., monitored environments such as retail stores and museums may already have WiFi access for customers who have bad cellular coverage, such WiFi accesses typically require additional log in steps or going through confirmation pop-ups which is an inconvenience to the user. Compared to this, our proposed system brings a seamless experience to the user.

    \item \textit{Very Long Lifetime:} Thanks to the promised multi-year broadcast lifetime of the BLE standard, which enables \Sys to last for a very long time (months, if not years) over battery power. In scenarios, where the dynamics of the scene is lower (e.g., monitoring a deserted area over a very long period) or motion is captured once but viewed many times (e.g., for post-fact analysis purposes), \Sys may remain vigilant for multiple years without a battery replacement.

    \item \textit{\rednote{External} Power Requirement:} Because of the phased operation, \Sys does not require \rednote{External power supply} during the read and render phase. This makes this system convenient and deployable in scenarios where supplying \rednote{external power supply} is a practical problem.

    \item \textit{Extensibility:} The system can be easily extended and tuned to provide extra functionalities. First, if a stable Internet access is available in the environment, more complex 3D structures and texture information can be downloaded to the viewer's smartphone at runtime to render a more vivid virtual object. Second, besides broadcasting 3D objects and visual features, a beacon can simultaneously broadcast additional information such as URLs and messages for other applications. In the other words, the purchased BLE beacons for the system can be used in other applications at the same time; this further reduces the overall cost.
\end{enumerate}

%\textbf{(a) Low cost} The entire system only requires about 8 BLE beacons and two Raspberry Pis attached with cameras. The cost can be as low as 150 dollars.

%\textbf{(b) Works in an Internet-free environment} Our system can work in any indoor environment where Internet access in not available. This brings a lot of values. This is because even though common use scenarios of our system such as retail stores and museums may have wifi access for customers to replace the low-signal cellular data usage, the wifi access typically requires a confirmation operation on a pop-up login web page, which brings inconvenience to users. Our AR system without an Internet connection would bring a seamless experience for a user.

%\textbf{(c) Long lifetime} Thanks to the long lifetime brought by the BLE standard, not counting the capture phase that requires DC power supply, our system can work years long without battery replacement.

%\textbf{(d) Does not require DC power supply in the broadcasting phase} In the capture phase of the system, the Raspberry Pi is turned on, which typically requires a power supply. But after that the captured 3D point data is stored in BLE beacons for broadcasting. Therefore, a DC power supply is not required. This further reduces the inconvenience and complexity in deploying the system.

%\textbf{(e) Easy for conversion / extension} The system can be easily tuned to provide other functionalities. For instance, if stable Internet access is available in the environment, a more complex 3D structure plus texture information can be downloaded from the Internet to a user’s smartphone. As a result, a more vivid virtual object can be rendered. Also, while broadcasting 3D mesh data and visual feature data that are required for our system, a BLE beacon can also simultaneously broad other common information such as URLs and messages for other applications. In the other word, the purchased BLE beacons for the system can be used in other applications at the same time; this further reduces the cost in total.


%The system contains four major functional components: capture, write, read and render. They are described in the four subsections below.

%Our compression routine consists of four stages as shown in Fig.~\ref{fig:image_processing_process}: multiple view capture, depth estimation, depth-refined segmentation and image compression. They are explained in the following four subsections.


% \begin{itemize}
%     \item Beacon Storage Constraints --- the goal is to store a) visual features for camera location and pose estimation, and b) captured person's 3D data. Parameters: beacons, delay/battery; optimize enough informative feature for a correct matching between Pi and user camera.
%     \item Viewer's Location and Pose Estimation --- the goal is to use 3 types of signals a) BLE signal strengths (sensitive to environment changes, fluctuates a lot, unreliable), b) IMU sensor (not good for location estimation), c) visual features (has restart problem or large data base). optimize: fast real-time, location or pose error is cm-level and 5 degrees.
% \end{itemize}

%\subsection{Capture}
%\subsection{Write}
%\subsection{Read}
%\subsection{Render}
