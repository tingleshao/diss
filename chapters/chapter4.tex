\chapter{AR}
\section{Introduction}
\label{sec:intro}

In this modern age of the Internet of Things (IoT), it is now possible to literally glue tiny computers to everyday objects, so that they can sense, react, and tell their own stories. The IoT community has embraced wireless standards such as Bluetooth Low Energy (BLE) and developed programmable `beacon' devices that periodically broadcast a small amount of preloaded data, while lasting for multiple years on a coin-cell battery. Broadcast messages from beacon devices typically contain information about an object, a location, a web-resource, or just an arbitrary string. This connectionless mode of BLE does not require a receiver to pair/bond or connect to a sender, and hence, there is no overhead of connection setup and no inconvenience of requiring a user to enter pins and passwords. These broadcast messages are received by a BLE capable mobile device to obtain relevant information just-in-time and on-the-spot.  Emerging applications of beacon devices include advertising merchandise in retail stores, identifying late passengers at the airports, authorizing people at the hospitals, smarter signage, indoor navigation, and tracking moving platforms like airline cargo containers, computers on wheels, museum artworks, or even humans.


The enabling technology behind these applications is the ability of a beacon to simply broadcast a few bytes of data (called UUID) as BLE 4.0 advertisement packets at a rate of less than 16 bytes/sec. The bound in data rate comes from the lifetime requirement of these devices. Such a tight budget on payload size and the maximum data rate have limited a beacon's capability to only be able to broadcast an identifier or a small amount of text (effectively $\sim$18 bytes). The next generation BLE 5.0 beacon is expected to have an 8X increase in broadcasting capacity ($\sim$256 bytes). Such an increase opens up the possibility to design beacons that can serve larger assets. We are particularly interested to know --- \emph{whether BLE beacons are capable of storing and broadcasting data structures describing 3D objects, so that nearby mobile devices are able to receive and render those virtual objects onto the real-world, and to have a seamless mobile augmented reality experience}. It turns out that the answer is --- \emph{yes}, but in designing such a system, we need to deal with at least two fundamental challenges. Before we dig into their details, let us look at the benefits of a beacon-based mobile augmented reality system.

There are several advantages of having an augmented reality system that consists primarily of a set of BLE beacons. First, the system will be \textbf{low cost.} Compared to today's \$3,000 augmented reality headsets, a beacon-based infrastructure will be several orders of magnitude cheaper. Second, beacons would store 3D objects locally and broadcast them over connection-less advertising channels \textbf{without requiring any Internet connectivity}. This makes the system simpler and in many cases hassle-free as cellular signals indoors is often unusable and connecting to free WiFi often requires accepting too many agreement pop-ups and occasionally watching an advertisement video. Third, beacons are designed to \textbf{last for a long time} with battery power, which makes these systems easier to setup and maintain when compared to wall power or frequent battery replacements. Fourth, in many modern buildings, \textbf{beacons are already in place}. Setting up a mobile augment reality system in those buildings would practically cost nothing.


%, e.g., simple html pages or thumbnail images.
%e.g., an image, carried by connectionless BLE advertisement packets. However, even a simple $72\times 72$ PNG image, such as the Android launcher icon, has a size of over 3KB. To store and broadcast this image, either we require to use a dozen of BLE 5.0 beacons, or we will have to accept a very long image transmission and loading time.

%Image compression is a natural way to deal with this problem. Existing image compression algorithms, however, fail to achieve the desired compression ratio for an image to be broadcasted over BLE. Hence, a fundamental challenge toward realizing an image beacon is to devise an algorithm that efficiently represents an image using as few bits as possible, while taking into account the application-driven limits on the number of usable beacons per image, broadcast message size, data rate, latency, and lifetime. In an earlier work~\cite{shaoyears}, we devised an image beacon system that broadcasts binary images of a few limited categories (e.g., handwritten characters) only. This paper is a continuation to that line of work, but this time, we have taken a harder challenge, i.e., to develop a beacon system that works for color images, e.g., images taken with a mobile phone.

The main challenges in any augmented reality systems are: 1) communication of 3D object data that includes visual features, textures, rendering information, and a time series of these objects in case of video, and 2) the location and pose of the viewer so that the object can be overlayed on the real-world at the correct location and orientation. Furthermore, in case of mobile augmented reality, as the user moves, the object needs to be reoriented and redrawn based on his current position. For a seamless experience, all these has to happen in real-time as well. BLE beacons, to some extents, provide support for both storage and localization. There have been ongoing investigation on BLE's capabilities to store complex data structures such as images~\cite{shaoyears}. Recently, Google started to experiment with an idea called \textit{`Fat Beacons'}, where they are looking into broadcasting html pages over BLE. Indoor localization and navigation using BLE signals~\cite{zhuang2016smartphone, martin2014ibeacon} is another active area of research. In this paper, we decide to combine these two promising aspects of BLE to enable more than what we have achieved with beacons so far.

%Emerging applications of beacon devices include advertising merchandise in retail stores~\cite{pierdicca2015low}, identifying late passengers at the airports, authorizing people at the hospitals, smarter signage, indoor navigation~\cite{martin2014ibeacon}, and tracking moving platforms like airline cargo containers, computers on wheels, museum artworks, or even humans~\cite{conte2014bluesentinel}. The enabling technology behind these applications is the ability of a beacon device to simply broadcast a few bytes of data (called UUID) as BLE advertisement packets at a rate of less than 16 bytes/sec. The bound in data rate comes from the lifetime requirement of these devices. Such tight budgets on payload and maximum data rate has limited a beacon device's capability to only be able to broadcast an identifier or a small amount of text (about 16--18 bytes). To transmit a moderate sized image, either we require to use hundreds of beacon devices, or we will have to accept a very long transmission delay.

%Hypothetically, if we could broadcast high-resolution images from a beacon device in real-time, the technology would enable even more powerful and feature rich applications. Like the web has evolved from serving hypertexts to streaming multimedia contents, we envision that the natural successor of a beacon device would be the one that broadcasts images, while meeting the same energy and lifetime requirement. Applications of such an image beacon system would be in scenarios where there is no Internet connectivity but there is a need for storing and broadcasting information that can be best described by an image. Potential applications of beacon image systems include coordinating rescue workers in disaster areas, creating a bread-crumb system for adventurous hikers and mountaineers, remote surveillance (when coupled with a camera), or even a simple system just to let someone know that `I was here'.


%Being able to broadcast images from beacons enables more powerful and feature rich applications than the ones supported by today's beacons. We envision that like the web has evolved from serving hypertexts to streaming multimedia contents, the natural successor of today's beacon devices would be the ones that broadcast images. Applications of image beacons would be in scenarios where there is no Internet connectivity but there is a need for storing and broadcasting information that can be best described by an image. For example, coordinating rescue workers in disaster areas, creating a bread-crumb system for adventurous hikers and mountaineers, remote surveillance (when coupled with a camera), or even a simple system just to let someone know that `We were here'.  Recently, Google started to experiment with an idea called \textit{`Fat Beacons'}, where they are looking into broadcasting html pages over BLE. However, for lack of a suitable image compression technique, the pages do not support images.

%Our work will complement such efforts.

%In this paper, we chase this seemingly impossible goal of creating a beacon device that efficiently broadcasts images over a long period. As a first step toward realizing an image beacon, we explore the challenges to broadcasting binary images of different categories (e.g., alpha-numeric characters, basic shapes, and arbitrary binary images), and design algorithms to efficiently store contents of an image inside a set of beacon devices. The set of beacons simultaneously broadcasts chunks of an image over BLE, which are captured by a mobile device to reconstruct the image. A fundamental challenge toward achieving this is to efficiently represent an image using as few bits as possible. Standard image compression algorithms are not good enough to archive the required compression ratio so that an image can be stored inside a beacon. We investigate image approximation/coding techniques that take into account the limits on number of beacon devices, number of bits available in a beacon device, data rate, latency, and lifetime. Based on empirical analysis, we devise a patch-based image approximation algorithm which greatly reduces the image data while keeping the image distortion under a threshold. We investigate the tradeoffs between the image quality and the power consumption to determine the best set of parameters for the system under user-specified constraints.

%In this paper, we chase this seemingly impossible goal of creating an image beacon system that efficiently broadcasts color images, carried by BLE broadcast messages, over an extended period of time. We propose a self-contained system that stores and broadcasts actual image contents as opposed to IDs, links, or URLs of an image. We assume availability of no additional information on the broadcasted image from any other sources -- globally (on the web) or locally (on a user's smartphone that receives the broadcast).

In this paper, we chase this seemingly impossible goal of creating the first mobile augmented reality infrastructure that is based primarily on a set of low-cost BLE beacon devices. Our target application is a motion capture scenario where a user (e.g. an actor, a doctor, or a lecturer) would enter into an area being monitored and make natural gestures while a distributed camera system would capture his motions. Later these captured movements can be replayed and viewed in 3D for various types of post-facto analysis purposes such as training and skill improvement.

We propose a self-contained system that consists of a cluster of BLE beacon units that are connected to an embedded micro-controller and a low-cost stereo camera. Once the system is installed and set up, it operates in two phases. The first phase is dedicated to detecting dynamic events in the area being monitored and to capture and store sufficient information abut moving objects or subjects in the scene. This information is compressed, stored, and broadcasting while meeting the storage and expected lifetime requirement of the system. The second phase is dedicated to receiving and rendering the 3D virtual objects and placing them at the right location and at right scale as a viewer moves and looks at the scene through his smartphone.

%The crux of the system is an algorithm that analyzes an image to identify its `important' semantic regions (as defined by the user or the use case) and then encodes them differently than the rest of the image to reduce the overall image size. The image data are written to and read from the image beacon system using a smartphone application, which runs the proposed compression and rendering algorithms. We use the term `beacon system' instead of `a beacon', since a compressed image may still require more than one physical beacons to ensure its acceptable quality. Allowing multiple beacons per image makes the system flexible. It widens our scope for optimizations and helps satisfy users who are willing to dedicate more beacons for better results. Besides, until BLE 5.0 is available, we need to simulate its broadcast capacity with multiple BLE 4.0 devices anyways.

The crux of the system are two algorithms that are central to the two phases of the system. The first of which intelligently determines 1) a least number of most informative and useful visual features, and 2) a minimal amount of information about the moving parts of a 3D object, and store this extremely compressed information into the limited storage of the beacons. The second algorithm utilizes the BLE signal strength and combines it with the user's smartphone's IMU and camera images to accurate estimate his position and the orientation in real-time.


%a standard JPEG image, converts it to binary format, and shows the user a preview of the compressed image to be written. The user is allowed to change the settings (e.g., the number of available beacons and/or expected device lifetime) and the app immediately shows the best possible compressed image under these constraints.

%We have developed a prototype of an image beacon system using a set of commercially available Estimote beacons~\cite{ESTIMOTE}, and developed an Android application that takes images of an object of interest along with user-specified requirements and constraints on broadcasting the image as inputs, generates previews of the image to be written, writes the image representation into a set of beacons, and reads the broadcasted image back. Figure~\ref{fig:beacons} shows an example scenario where a user snaps photos of a gnome statue which he is interested in broacasting. The smartphone application performs image processing on the phone to produce multiple versions of broadcast image. The user selects one of these compressed images that satisfies his requirements (e.g. available beacons, image quality, lifetime, and image loading latency). The user is allowed to change his requirements and the app immediately shows options for the best possible compressed images under those constraints. The application writes the image data into the beacon system and the image is broadcasted by the beacons. A reader application reads the broadcasted image and displays it on the phone.

We have developed a prototype of the system, called the MARBLE, that consists of eight Estimote beacons~\cite{ESTIMOTE} connected to eight Raspberry Pis~\cite{RPI}, each having two Arducam~\cite{ARDUCAM} cameras attached to it. The prototype has been thoroughly tested to quantify its CPU and memory usage, as well as the accuracy of feature selection and localization algorithms. We demonstrate MARBLE by setting up an indoor motion capture scenario where 10 volunteers make five types of gestures for about three seconds while the system captures and stores their motions. Later, they enter the scene, walk around, and view the captured actions in 3D through their mobile phones.

The main contributions of this paper are as follows:


%We perform an in-depth evaluation of the beacon system. We describe a set of results showing the tradeoffs between system lifetime and image quality, when the image type and the number of beacons are varied. We also deploy an image beacon system indoors, and perform a user study in a real-world scenario in order to have a subjective measure of the quality of the received images, where a group of $20$ participants are asked to identify objects from their beaconed images of various resolutions, and locate it among a set of similar looking objects in the real-world.

%\begin{itemize}[leftmargin=10pt]
	%\vspace{-0.25em}
	%\item To the best of our knowledge, we are the first to propose a BLE beacon-based mobile augmented reality system.

	%\vspace{-0.25em}
%	\item We devise two algorithms: 1) an algorithm that determines a least number of most informative and useful visual features, and a minimal amount of information about the moving parts of a 3D object, and 2) an algorithm that utilizes the BLE signal strength and combines it with the user's smartphone's IMU and camera images to accurate estimate his position and the orientation in real-time.

%	\item We have developed and evaluated a prototype of the proposed system. Our evaluation shows that the system takes about 170ms to capture an object's motion, 613ms to render the scene, the selected features are 95\%-100\% accurate in determining the reference view, and the mean localization error is 14.5 cm.

%	\item We conduct a user study involving 10 participants and demonstrate that they system is capable of capturing free hand gestures and when replayed back, the users were able to view and experience them in real-time.

%\end{itemize}

\section{Problem Formulation}
\label{sec:problem}

\subsection{Generic Problem Setting}

The problem is formally stated as: given an image $\mathrm{x}$ (where each pixel is represented by $\mathrm{b}$ bit) having the dimensions of $\mathrm{N \times M}$, the number of available beacon devices $\mathrm{K}$, the payload size of each beacon packet $\mathrm{C}$ bytes, the maximum allowable broadcast rate of $\mathrm{R}$ packets/sec, and the maximum allowable latency for an image $\mathrm{T}$, the objective is to find an approximate representation of the image $\mathrm{\hat{x}}$ so that the lifetime $\mathrm{\tau}$ of the beacon system is maximized while the approximation ratio $\mathrm{\lambda(x, \hat{x}) \in [0,1]}$ of the image is high ($\mathrm{\lambda = 1}$ means no distortion). Now, for a single beacon, the broadcast rate:
\begin{equation}
	\mathrm{R = \bigg( \frac{bNM}{8C} \bigg) \frac{1}{T}}
	\label{eq:1}
\end{equation}

For K beacons, considering $\log K$ overhead bits for addressing the beacons, and $K$ times more payload capacity:
\begin{equation}
	\mathrm{R = \bigg( \frac{bNM + \log K}{8CK} \bigg)  \frac{1}{T}}
	\label{eq:2}
\end{equation}

Both ~(\ref{eq:1}) and ~(\ref{eq:2}) are for undistorted images.



The lifetime $\mathrm{\tau}$ of a BLE device depends on its inter packet interval and in general, $\mathrm{\tau \propto \frac{1}{R}}$. Replacing $\mathrm{R}$ and incorporating approximation ratio $\mathrm{\lambda}$ into~(\ref{eq:2}):

\begin{equation}
	\mathrm{\frac{1}{\tau} \propto \bigg( \frac{\lambda bNM + \log K}{8CK} \bigg)  \frac{1}{T}}
	\label{eq:3}
\end{equation}

The above equation relates the lifetime of an image beacon system and the approximation ratio of any image compression algorithm. Ideally, we look for an image approximation algorithm that achieves a sufficiently large $\mathrm{\lambda}$ for a reasonably high lifetime of the system.



\subsection{Broadcast Capacity of Bluetooth LE}

According to the BLE 4.0 specification, the maximum payload size $C$ available in beacons is 18 bytes. However, there are 33 reserved characters that cannot be read from the beacon devices. So, practically the payload size is $\lfloor \log (256-33)^{17} \rfloor \approx 132$ bits $\approx 16$ bytes.

The expected lifetime of BLE beacons depends on the inter-packet interval~\cite{dementyev2013power}. For example, a BLE 4.0 beacon would last up to 3.5 years, if a packet is sent at every second (i.e. R = 1). Therefore, for a beacon system to last for 3.5 years, its broadcast bandwidth cannot exceed 16 bytes/sec.

Recently, BLE 5.0 has been announced~\cite{BLE5} to offer an 8X increase in broadcast capacity and a 2X increase in transmission speed. It is scheduled to be released in early 2017. In coming days when BLE 5.0 capable devices will be widespread, we expect to have a $128$ byte sized payload and about 256 bytes/sec broadcast bandwidth.

\subsection{The Case for Loss-Less Image Broadcast}

The size of a typical $72\times 72$ PNG image can be anywhere between $3-13$ KB. Therefore, to transmit such an image, a BLE 4.0 beacon would require $191-832$ broadcast packets, or alternatively, we would require up to $K = 832$ beacons to simultaneously broadcast different slices of an image. The latency of a complete image transmission cycle would be up to $\mathrm{T = 13.9}$ minutes for a single beacon, or 1 second for a set of $832$ beacons.

When BLE 5.0 beacons will replace 4.0, the transmission latency will drop to $52$ seconds for one beacon, or 1 second 52 of them. Therefore, without compressing the image content, even the new BLE 5.0 beacons will not be able to support a fast image beacon system with a reasonably small number of beacons.

%Recall that our goal is to have an image compression routine that fit the beacon systems limited storage space.

\subsection{The Case for Compressed Image Broadcast}

If standard image compression algorithms could generate compressed images that meet the size and quality requirements of an image beacon system, the problem would have been already solved. But the fact is, even the best of existing image compression methods, such as JPEG/JPEG2000 and PNG, are not capable of optimizing for both quality and size at the same time. Figure~\ref{fig:common_codec_comparison} illustrates that JPEG/JPEG2000 generates extremely poor quality images given a size requirement of 300 bytes even for a very low-resolution ($64 \times 64$ pixels) image. On the other hand, to have a compressed image of acceptable quality (having a minimal useful visual information to the viewer), JPEG/JPEG2000 takes about 2K bytes.

% [XXX mention encoder used]. For images compressed in both codec, without having looked at the high quality version of the image, it is hard to recognize what is in the low quality version of the image.
%\begin{figure}[!htb]
%    \begin{center}
	%    \includegraphics[width=0.49\textwidth]{img/common_codec_comparison.pdf}
	%    \vspace{-2em}
	%    \caption{\footnotesize A 64x64 resolution image compressed in high/low quality settings using JPEG/JPEG2000: (a) JPEG high quality, 1963 bytes (b) JPEG2000 high quality, 2026 bytes (c) JPEG lowest possible quality, 738 bytes (d) JPEG2000 lowest possible quality, 391 bytes.}
	%    \label{fig:common_codec_comparison}
%    \end{center}
%\end{figure}

%\begin{figure}[!htb]
%    \begin{center}
%    	\vspace{-1em}
	%    \includegraphics[width=0.25\textwidth]{img/common_codec_comparison2.pdf}
	%    \vspace{-1em}
	%    \caption{\footnotesize Two types of 64x64 resolution image compressed in PNG (a) from natural scene, 12112 bytes (b) JPEG2000 high quality, 1012 bytes. PNG is good for handling images with large uniform color regions. }
	%    \label{fig:png_block}
	%    \vspace{-1em}
%    \end{center}
%\end{figure}

PNG and Vector Graphics image, on the other hand, have the potential to generate a smaller compressed image that \textit{may} fit our constraints. However, these codecs generate smaller images only if the input image is of a specific type -- such as an image containing a few regions of uniform colors like a cartoon drawing, or when the shape is not complicated. This is illustrated in Figure~\ref{fig:png_block}. In general, PNG and Vector Graphics image encoding do not meet the requirements of an image beacon system that broadcasts color images taken by a smartphone user.

%But we envision an image beacon system that would able to store and broadcast images taken from the real world using a user's smarptonhe camera. In general, PNG and Vector Graphics image encoding do not meet the requirements for broadcasting those types of images.

\section{Overview of \Sys}
\label{sec:system}


The \Sys is a mobile augmented reality system that uses a cluster of off-the-shelf, low power, storage and bandwidth constrained Bluetooth Low Energy (BLE) beacon units as an infrastructure. The system operates in two phases: 1) the capture and store phase, and 2) the read and render phase.

Figure~\ref{fig:beacons} (left) shows that a person \rednote{enters} an area being monitored with \Sys. A set of cameras capture his actions and stores them inside beacons. Later (right), his activity during the first phase is viewed through a 3D augmented reality application on a mobile phone. \rednote{Due to the low energy consumption feature of BLE protocol, the broadcasting phase is extremely energy efficient. In most application scenarios of \Sys, capture phase is only performed once, or the capture is triggered by a rare event that only happens every few hours or few days. Therefore, in most of the time, \Sys is working in the broadcasting phase and the expected system lifetime can be more than 6 years.}

\begin{figure}[!thb]
	\begin{center}
		\includegraphics[width = 0.49\textwidth]{img/fig1.png}
		\caption{The two phases of \Sys indoor augmented reality system: capture (once) and render (many times).}
		\label{fig:beacons}
	\end{center}
	\vspace{-1em}
\end{figure}

\begin{figure*}[!htb]
	\begin{center}
		\includegraphics[width = \textwidth]{img/photo2.png}
		\caption{System in Action. (a): in capture phase, a person's gesture and movement is captured. (b): in render phase, the virtual object avatar is rendered in the empty environment. (c): screenshot of the viewer's screen in render phase.}
		\label{fig:run_pi}
	\end{center}
\end{figure*}

\begin{figure*}[!htb]
	\begin{center}
		\includegraphics[width = \textwidth]{img/overview.pdf}
		\caption{A block diagram illustrating the work flow of \Sys.}
		\label{fig:overview}
	\end{center}
	\vspace{-1.5em}
\end{figure*}

\subsection{Two Phases of \Sys}

During the \emph{capture and store} phase, a set of low-power cameras are used to capture the dynamics of a scene. To simplify the design of each unit, we attach a stereo camera to each BLE beacon via a low-power embedded microcontroller. However, this is not a requirement in \Sys for every BLE beacon to have its own camera, i.e. a camera can be shared by multiple beacons. We assume that the duration of dynamic events during this phase is relatively shorter, and hence, the energy consumption due to active cameras is not excessively high. Once the microcontroller computes the minimal necessary information about the scene, and the 3D objects and their motions to store, data is sent to the beacon for broadcasting over the connectionless BLE advertisement packets.

During the \emph{read and render} phase, a mobile device such as a smartphone or a augmented reality headset with  a BLE capability, receives the broadcasts and renders the previously captured scene in 3D. A major task in this phase is to determine the position and orientation of the mobile device, so that the objects are rendered at the right position and orientation in the superimposed space created by the virtual and the real environment as the users walks in the space. To achieve this, the received signal strengths of the BLE beacons, in combination with the IMU and camera of the viewer's mobile \rednote{device}, is used.



%The two phases of \Sys are illustrated in Figure~\ref{fig:beacons}. For example, if a person enters the monitored area, his motion is captured and stored


%This section describes an AR indoor surveillance 3D capture and replay system. The system is a realization of our idea of AR with BLE beacons. To the best of our knowledge, we are the first to illustrate an end to end BLE beacon based system with augmented reality 3D rendering.

\subsection{Internal Modules and Basic Workflow}

During the set up of \Sys, a number of beacon units are placed at fixed positions in an indoor environment surrounding the area to be monitored. Assuming a camera is attached to each beacon unit, each beacon takes pictures of the empty space -- capturing only the environmental artifacts such as interesting points on the wall, furniture, and portions of the floor and the ceiling it sees. Each beacon then computes and stores a unique set of visual features (\emph{`oriented fast and rotated brief'} (ORB) features~\cite{rublee2011orb}) of the view it observes. This view serves as the baseline view for a beacon, and is later used during the view tracking of the mobile user.

%each beacon has a set of visual features (in our case, ORB~\cite{ref} features) stored. Every beacon contains a unique set of visual features that are extracted from an image taken by a camera placed at the location of the beacon. Features of these images are later used to match the view of a user to track his location during the read and render phase.

The workflow of our system is illustrated in Figure~\ref{fig:overview} along with the internal modules for each of the two phases of \Sys.

The capture and store phase starts with the detection of any kind of change in the scene being monitored, e.g., when a subject enters into the area or an already present object or the subject moves. This awakens the cameras attached to the beacons which starts capturing and processing image. Processing the image stream involves simple predefined tasks such as detecting predefined interesting artifacts like a human body. If an object of interest is detected, key points corresponding to its moving parts (e.g., heads and hands in case of a person) are recorded over time inside the beacon storage.

%a person enters the room where the system is deployed. The multiple shooting cameras attached to Raspberry Pi running pre-defined interesting artifacts such as human body detection programs then detect the person. The program running on Raspberry Pi locates the person in the video frame, detects the head and hands of the person, and stores the series key points describing the locations of head and hands over time into the beacons.

During the rendering phase, each beacon broadcasts its unique set of visual features and a subset of the captured 3D point time series data. At the receiving end, the viewer's smartphone receives two types of information from all beacons: 1) broadcasted visual features, and 2) received signal strengths. The smartphone combines this information with its own view as seen from its own camera to obtain a BLE-aided image-based location and pose estimate of its own. This is further combined with the smartphone's IMU data to accurately determine the position and orientation of the smartphone. Finally, location data and 3D object data are used to render the 3D object on his smartphone in real-time.

%The data is received by a viewer's smartphone in the room. The visual features, along with the beacon signal strength and the smartphone's IMU values are used in estimating the smartphone’s camera position and pose. Then based on the estimation, the 3D object that represents a moving person is rendered on the smartphone’s screen along with the camera preview.

\subsection{Advantage of \Sys}

There are several \textcolor{red}{advantages} of using \Sys compared to existing mobile augmented reality systems:

\begin{enumerate}
    \item \textit{Low Cost:} \Sys is built upon low-cost technologies. The current prototype, consisting of eight Estimote BLE beacons, two Raspberry Pis with Arducam cameras, cost about \$150 and is sufficient to monitor an indoor space of \rednote{6.5m $\times$ 5.3m}. The whole setup is 6--20 times less costly than existing augmented reality headsets~\cite{vuzix, hololens}.

    \item \textit{Internet Free:} \Sys relies only on Bluetooth connectivity. It works in indoor environments where Internet access in unavailable or an inconvenience. Being Internet-free has some advantages; e.g., monitored environments such as retail stores and museums may already have WiFi access for customers who have bad cellular coverage, such WiFi accesses typically require additional log in steps or going through confirmation pop-ups which is an inconvenience to the user. Compared to this, our proposed system brings a seamless experience to the user.

    \item \textit{Very Long Lifetime:} Thanks to the promised multi-year broadcast lifetime of the BLE standard, which enables \Sys to last for a very long time (months, if not years) over battery power. In scenarios, where the dynamics of the scene is lower (e.g., monitoring a deserted area over a very long period) or motion is captured once but viewed many times (e.g., for post-fact analysis purposes), \Sys may remain vigilant for multiple years without a battery replacement.

    \item \textit{\rednote{External} Power Requirement:} Because of the phased operation, \Sys does not require \rednote{External power supply} during the read and render phase. This makes this system convenient and deployable in scenarios where supplying \rednote{external power supply} is a practical problem.

    \item \textit{Extensibility:} The system can be easily extended and tuned to provide extra functionalities. First, if a stable Internet access is available in the environment, more complex 3D structures and texture information can be downloaded to the viewer's smartphone at runtime to render a more vivid virtual object. Second, besides broadcasting 3D objects and visual features, a beacon can simultaneously broadcast additional information such as URLs and messages for other applications. In the other words, the purchased BLE beacons for the system can be used in other applications at the same time; this further reduces the overall cost.
\end{enumerate}

%\textbf{(a) Low cost} The entire system only requires about 8 BLE beacons and two Raspberry Pis attached with cameras. The cost can be as low as 150 dollars.

%\textbf{(b) Works in an Internet-free environment} Our system can work in any indoor environment where Internet access in not available. This brings a lot of values. This is because even though common use scenarios of our system such as retail stores and museums may have wifi access for customers to replace the low-signal cellular data usage, the wifi access typically requires a confirmation operation on a pop-up login web page, which brings inconvenience to users. Our AR system without an Internet connection would bring a seamless experience for a user.

%\textbf{(c) Long lifetime} Thanks to the long lifetime brought by the BLE standard, not counting the capture phase that requires DC power supply, our system can work years long without battery replacement.

%\textbf{(d) Does not require DC power supply in the broadcasting phase} In the capture phase of the system, the Raspberry Pi is turned on, which typically requires a power supply. But after that the captured 3D point data is stored in BLE beacons for broadcasting. Therefore, a DC power supply is not required. This further reduces the inconvenience and complexity in deploying the system.

%\textbf{(e) Easy for conversion / extension} The system can be easily tuned to provide other functionalities. For instance, if stable Internet access is available in the environment, a more complex 3D structure plus texture information can be downloaded from the Internet to a user’s smartphone. As a result, a more vivid virtual object can be rendered. Also, while broadcasting 3D mesh data and visual feature data that are required for our system, a BLE beacon can also simultaneously broad other common information such as URLs and messages for other applications. In the other word, the purchased BLE beacons for the system can be used in other applications at the same time; this further reduces the cost in total.


%The system contains four major functional components: capture, write, read and render. They are described in the four subsections below.

%Our compression routine consists of four stages as shown in Fig.~\ref{fig:image_processing_process}: multiple view capture, depth estimation, depth-refined segmentation and image compression. They are explained in the following four subsections.


% \begin{itemize}
%     \item Beacon Storage Constraints --- the goal is to store a) visual features for camera location and pose estimation, and b) captured person's 3D data. Parameters: beacons, delay/battery; optimize enough informative feature for a correct matching between Pi and user camera.
%     \item Viewer's Location and Pose Estimation --- the goal is to use 3 types of signals a) BLE signal strengths (sensitive to environment changes, fluctuates a lot, unreliable), b) IMU sensor (not good for location estimation), c) visual features (has restart problem or large data base). optimize: fast real-time, location or pose error is cm-level and 5 degrees.
% \end{itemize}

%\subsection{Capture}
%\subsection{Write}
%\subsection{Read}
%\subsection{Render}

\section{Implementation Notes}
\label{sec:impl}

We developed each unit of \Sys using commercial, off-the-shelf components. Figure~\ref{fig:sys} shows one such unit.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width = .4\textwidth]{img/Beacon_pic1}
		\caption{The capture unit. \rednote{Font size too big.}}
		\label{fig:sys}
	\end{center}
\end{figure}

Raspberry Pi is a card-sized single board low cost computer. We are using the Raspberry Pi 3 model which has Quad Core Broadcom BCM2837 64-bit ARMv8 processor with up to 1.2GHz, 1 GB RAM and 40 GPIO pins. The dimension  and weight of Raspberry Pi 3 is 86.9mm x 58.5mm x 19.1mm and 1.5oz. We are using Arducam OV5647 Mini Camera Module for Raspberry Pi. As Pi 3 is equipped with one Camera interface (CSI), we are using Arducam Multi Camera Adapter Module to interface stereo camera.


%Device
In all of our experiments, we use Estimote model REV.F2.3 Radio Beacon~\cite{ESTIMOTE} having a 32-bit ARM Cortex M0 CPU, 256 KB flash memory, 4 dBm output power, 40 channels (3 for advertising), and 2.4-2.4835 GHz operating frequency. The rendering application is written in Java/C++ and runs in a Nexus 5 smartphone having a 2.26GHz quad-core Qualcomm Snapdragon 800 processor, 2 GB RAM, BLE v4, and \rednote{Andriod 6}. It uses OpenGL ES and ARToolkit library in rendering the virtual object. It uses OpenCV library in camera based pose estimation and localization. We mimic BLE 5.0 broadcast packets by a set of rolling BLE 4.0 packets. The rolling mechanism is implemented by configuring the Estimote Location beacons to broadcast customized advertising packets.
%The customized data is received from an Android compatible LightBlue Bean device~\cite{LBB} via the beacon's GPIO, configured as an UART interface.


\section{Evaluation}
\label{sec:eval}

In this section, we describe a series of empirical evaluations. We designed experiments to cover the evaluations for both the capture and rendering phases. \rednote{We set up an experimental environment in our lab with size 6.5m $\times$ 5.3m. This is a typical use case scenario for our indoor AR system.} For the capture phase, we evaluated the performance of visual feature selection. For the rendering phase, we evaluated the performance of reference camera matching in initialization. We also evaluated the system localization and pose estimation accuracy. In the end, we performed a user study to discover the trade off between the size of 3D object of interest data and the level of preservation of the object motion information captured by the system.

\subsection{Microbenchmarks}

Figure~\ref{fig:run_pi} shows the execution times of different components of the \sys system. \rednote{The capture phase of the system runs on Raspberry Pi. The render phase executes on Nexus 5 Android phone}. Detecting moving objects takes 65 ms and identifying point of interest takes 103 ms. Obtaining 3D location takes only 1 ms in average. Both rendering, BLE ranging and IMU data processing takes less than 1 ms in average. The most time expensive process is camera data processing, which takes \rednote{611} ms.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width = 0.45\textwidth]{img/benchMark}
		\caption{Run-time Analysis}
		\label{fig:run_pi}
	\end{center}
\end{figure}


\rednote{We also perform an energy analysis of the system. The result is shown in Figure~\ref{fig:energy1}. The energy consumption of different stages of the system is listed. The analysis is based on the setting of 1 s object of interest capture time, 1-year data broadcasting and 15-second viewing. The data consists of 15 frames of a virtual object motion. For the data broadcasting stage, the Beacon is configured with the broadcasting power -10 dBm and advertising interval 500 ms. }

\rednote{We consider the fact that different components in \Sys operates in different time period. To better understand the impacts of the energy consumption in each stage on the overall system, we compute the average energy cost in each component for delivering one frame (one set of key points positions) of virtual object data to the viewer. Even though BLE Beacons (green bar) operates at a much lower power than Raspberry Pi capture unit and Android viewer device because it is expected to stay in broadcasting mode for a year, it consumes most energy comparing to other components in \Sys.}

\rednote{We would also like to know the impact of changing the frequency of \Sys's capturing operation on the system's lifetime. By fixing the amount of energy the system can consume, we compute the expected lifetime of the system under a variety of capture settings. The result is given in Table~\ref{tab:lifetime_estimation}. We set the total consumable energy to be 25000 mJ. Because the viewer's device is usually powered by a separate battery, and the number of viewers in the environment is not fixed. We don't include the viewer's power consumption in this analysis. }

\rednote{The result suggests that, when our system works in an active surveillance mode that captures the object of interest in the environment every minute, the system is expected to last for 30 days. When we only capture once at the beginning and then keep broadcasting without re-capturing, the system can work nearly 7 years.}

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width = 0.48\textwidth]{img/energy1.png}
		\caption{Energy Consumption of every stage of \sys. The data for the capture phase is colored in blue. The data for broadcasting phase is colored in green. The data for viewing phase is colored in orange. The power data next to the stage labels are the working power of each stage. The number next to the bar charts are the energy consumption for rendering a frame of the virtual object.}
		\label{fig:energy1}
	\end{center}
\end{figure}

\begin{table}[!thb]
\centering
\footnotesize
\resizebox{.45\textwidth}{!}{
\begin{tabular}{r|c}
\textbf{Capture Frequency}   &  \textbf{Expected Lifetime}  \\ \hline
No capture (Only Broadcasting)  & 6 years and 348 days \\
Once in a lifetime & 6 years and 348 days \\
Once every day  & 6 years and 207 days     \\
Once every hour  & 2 years and 325 days    \\
Once every minute  & 30 days
\end{tabular}
}
\caption{System lifetime under different settings.}
\label{tab:lifetime_estimation}
\end{table}

Table~\ref{tab:cpu_mem_usage} lists the CPU and memory usage for the corresponding process in the pipeline in capture phase. For this phase the most expensive process is detecting moving object which takes 55.6\% CPU and 5.4\% memory. Identifying points of interest and obtaining 3D location takes 17.6\% and 1.3\% CPU, respectively. Table~\ref{tab:cpu_mem_usage_sp} gives an overview of the CPU and memory usage for the components of second phase. Here, IMU data processing obtains most user CPU, which is 16.33\%. Though camera data processing requires lowest user CPU (4.22\%), it uses most memory (52.11 MB) among these three.

\begin{table}[!thb]
\centering
\footnotesize
\resizebox{.4\textwidth}{!}{
\begin{tabular}{r|c|c}

&  \textbf{CPU (\%)} & \textbf{Memory (\%)}   \\ \hline
Detect Moving Objects     & 55.6                    & 5.4 \\
Identify Points of Interest  & 17.6                 & 5.4     \\
Obtain 3D Locations  & 1.3               & 2.5
\end{tabular}
}
\caption{CPU and memory usage of Raspberry Pi}
\label{tab:cpu_mem_usage}
\end{table}
\vspace{-0.5cm}

\begin{table}[!thb]
\centering
\footnotesize
\resizebox{.48\textwidth}{!}{
\begin{tabular}{r|c|c|c}

&  \textbf{User CPU} & \textbf{Kernel CPU} & \textbf{Memory}   \\
&  \textbf{(\%)} & \textbf{(\%)} & \textbf{(MB)}   \\
\hline
Rendering \& BLE Ranging     & 6.55      & 3.29              & 65.3   \\
IMU Data Processing  & 16.33       & 0.35             & 5.27     \\
Camera Data Processing  & 4.22     & 2.02           & 52.11     \\
\end{tabular}
}
\caption{CPU and memory usage of Smartphone}
\label{tab:cpu_mem_usage_sp}
\end{table}
\vspace{-0.5cm}


\subsection{Algorithm Evaluation}
\subsubsection{Visual Feature Selection Performance}

We evaluate the our feature selection approach described in section 3.1. We compare our method with the approach of picking features randomly. We define the metric of this evaluation by dividing the number of correctly matched features with the total number of candidate features from the viewer's cameras.

In the experiment, we fix the number of features from the reference camera to be 4. We use 9 images taken by the viewer's camera. For each image taken by the viewer's camera, we record the number of correctly matched feature points with the reference camera's features selected using our method and reference camera's features selected randomly. We performed the same experiment \rednote{twice with two sets of images taken in two} lab spaces with different lighting conditions. We plot the result in Fig.~\ref{fig:feature_accuracy}

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width = 0.45\textwidth]{img/AccuracyLab}
		\caption{Averaged accuracy of matching between ORB features found in 10 query images and the 4 ORB features from the reference image.}
		\label{fig:feature_accuracy}
	\end{center}
	\vspace{-1.5em}
\end{figure}

The result shows that our method can significantly improve the feature matching performance, given the smallest possible number of features to be stored, our method gives 95\% matching accuracy in lab 1 (darker room) and 100\% accuracy in lab 2 (brighter room). As a comparison, randomly picking features gives 77.5\% and 65\% chance of generating a correct result in two rooms, respectively.

\subsubsection{Beacon-Guided Camera Matching Performance}
We evaluate the performance of Beacon-Guided Camera Matching in the initialization stage. In setting up the experiment, we placed 64 measurement spots on the floor of the environment, as illustrated in Fig.~\ref{fig:lab} (a) After that, the 8 beacon's Bluetooth signals' strength are measured and the BLE beacon based location estimation is performed. Then the reference cameras that are far away from the viewer is filtered out. The number of reference cameras being considered at each measurement spots is shown in Fig.~\ref{fig:lab} (b). We can observe that in all measurement points, at least one reference camera is excluded. In half of the cases, more than 3 reference cameras are excluded. This means reduction of computation time by almost 40\% and increase of potential matching success rate.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width = .45\textwidth]{img/lab.png}
		\caption{(a): the configuration of measurement points in the lab. (b): heat map showing the number of cameras being considered in visual feature matching. The right button corner is for without using Beacon-Guided Camera Matching (using all 8 cameras).}
		\label{fig:lab}
	\end{center}
	\vspace{-1.5em}
\end{figure}

\subsubsection{Localization Accuracy}

We conduct an experiment to measure the localization and pose estimation accuracy of MARBLE system in the render phase. We designed a viewer trajectory that has a length about half of the room size. We set 16 measurement points along the trajectory. \rednote{We fix the viewer camera's angle and location at every measurement point. We record the angle and location of every measurement point as the ground truth. In the experiment, we move the viewer device along the trajectory, at each measurement point. We carefully move the viewer's device so device's location and viewing angle match the designed value at that point. At every measurement point, we record the camera state estimation (IMU-only, BLE-only, camera-only and the Kalman filter joint estimation). We designed the experiment to have two measurement points with viewer's device not pointing to the direction of the virtual object. At these two points, the system turns off the camera based estimation. So the estimation error from these two points for the camera-only method is not available (marked as crossed in Figure~\ref{fig:location_heatmap} and Figure~\ref{fig:angle_heatmap}). In the analysis, we compute the localization error based on square root of the sum of squared differences in $x$, $y$ and $z$ between estimate and ground truth.}

We plot the localization result error in Figure~\ref{fig:location_heatmap}. The result suggests that the camera based localization has a higher accuracy than BLE based localization in most cases. The sensor fusion using both BLE signal and camera signal provides an even more accurate result, except for the first measurement point, where the camera based method has the highest accuracy.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width = 0.48\textwidth]{img/location_colorbar2.png}
		\caption{Heat Map of the errors in cm on location estimation in 16 different measurement points. From left to right: BLE Beacon based estimation, Camera based estimation and Sensor Fusion using two signals.}
		\label{fig:location_heatmap}
	\end{center}
	\vspace{-1.5em}
\end{figure}

\subsubsection{Pose Estimation Accuracy}
We also plot the accuracy of pose estimation for the experiment described in the previous subsection. We derive the error by computing square root of the sum of squared difference on all rotation angles between the ground truth and the estimated values.

The result suggests that the IMU based pose estimation has a slightly higher accuracy than camera based estimation. The error stays less than 20 degrees.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width = 0.45\textwidth]{img/angle_colorbar.png}
		\caption{Heat Map of the errors in angle on pose estimation in 16 different measurement points. From left to right: IMU based estimation, Camera based estimation and Sensor Fusion using two signals.}
		\label{fig:angle_heatmap}
	\end{center}
	\vspace{-1.5em}
\end{figure}

\subsubsection{Real Deployment and User Study}

We perform a real user study to explore the performance of the entire system. We design the experiment with 10 participants entering the room one by one. We captured videos of a human doing five gestures at two different speed in the room. The five gestures are: left hand wave, right hand wave, clap, hand roll, and both hand wave. The gesture was completed within 1.5 seconds and 0.75 seconds for low speed and high speed respectively. We then detect the head and hands location in 3D to generate the key points over time. We sampled the points at 3Hz, 6Hz, 9Hz, 12Hz and 15Hz to generate data with different size.

In the render phase we use these data to create our moving virtual object (avatar). Once a user enters the room we show him/her the avatar performing one of the five gestures on the phone running the render app. The user then guess which gesture is being performed. We vary the avatar data being used with different sampling frequency and speed. In total we gather 50 answers. Based on the correctness of the answers we derive the quality of the avatar at every sampling frequency and speed combination.

\begin{figure}[!htb]
	\begin{center}
		\includegraphics[width = 0.45\textwidth]{img/User2}
		\caption{Human Gesture Sampling Frequency Impact at (a) Low Speed (b) High Speed.}
		\label{fig:user}
	\end{center}
\end{figure}

In Figure~\ref{fig:user}(a), we can see that for low speed gesture, 6Hz sampling frequency is enough (100\% accuracy) for users to recognize the easy gestures like right hand wave and left hand wave. Although for more difficult gestures like both hand wave, it gives about 75\% accuracy. In Figure~\ref{fig:user}(b), we can see that higher gesture speed has a higher recognition accuracy (more than 90\%) for difficult gestures at 9Hz, it gives lower accuracy (70\%) of easy gestures at 6Hz. Overall, we argue that 9Hz sampling frequency is good enough to capture the meaning of a motion around 1s. This requires 162 bytes data, which can be broadcasted by one BLE 5 packet.
% \\
% \rnote{From this experiment, we can conclude that having 9Hz sampling rate gives us at least 90\% accuracy.}

\section{Conclusion}
\label{sec:conclusion}
In this paper, we described MARBLE, an Internet-free augmented reality system that uses beacon devices. The system captures the object of interest in the environment, find the 3D key points that best describes the object of interest given the limited storage, and transmit it to viewer for rendering. The viewer uses multiple sensor information, including he beacon BLE signal strength, to determine its location and pose in the environment. \rednote{In the broadcasting phase}, the MARBLE system works without \rednote{external} power support and battery replacement for years.

The main contribution of our work is the overall system design and construction, the method of selecting the best visual features for camera-based localization and pose estimation and quantifying the trade-off between object key point data size and the quality of the object approximation. Our work widens the usage of the energy efficient, long-lasting beacon devices by enabling the application of this type of devices for indoor AR application. \rednote{Our system can be easily extended for building other types of AR applications including real world object tagging.}

% XXX
% Our system will perform at its best with the beacons that adopts the upcoming Bluetooth 5.0 standard. A future work is to evaluate the different aspects of the system performance as the Bluetooth 5.0 is released and gets popular. Moreover, a `fat-beacon' standard is under development at Google, that allows an even higher broadcast transmission capacity for BLE beacons. The goal of that standard is to equip beacon devices with the ability to broadcast basic web contents to smartphones in absence of the Internet connectivity. It will be meaningful to study the application of our image beacon system combined with a fat beacon.

\section{Discussion}
\label{sec:discussion}

The system is a realization of using BLE beacons for indoor augmented reality. Start from MARBLE, several possible extensions can be developed and research can be conducted.

In the capture phase, if the object of interest is a human body, with a face recognition and face tracking technique, the accurate orientation of where the person is facing to can be estimated. And the face details can be captured. Therefore the additional face information can be stored and broadcasted in the system. \rednote{This will enable a visualization of the face as a part of the avatar}. Besides that, the entire shape information may be captured and represented as a 3D mesh, to replace the key points information. A mesh compression method such as~\cite{DRACOBLOG} or~\cite{valette2004wavelet} that reduces the 3D mesh data size can be applied.

To enhance the user experience with an avatar that contains more details of the object of interest, another approach is to store both the texture information and shape information. The challenge for this is that the texture information usually requires thousands of bytes storage space, which exceeds the system's storage capacity. One custom image compression techniques that designed for such extreme low storage case is described in~\cite{shaoyears}, that algorithm could be extended to better support textures.

\rednote{Depending on the viewer's device, more types of input signals including GPS or infrared camera can be added into the sensor fusion model. \Sys should make use of the three input signals described in this paper as the basis for the localization and pose estimation task, and it should be able to accommodate new types of sensor inputs when they are available.}
% Another possible direction of extending the system is to explore the ability in supporting multiple objects of interest that has interactions. For example, capturing two people's movement and their geometric relationship in the environment could be studied.
% The current MARBLE system has a regular capture unit configuration. In some environments that is less regular, such as a non-rectangular shape room, the optimal way of deploying the capture unit to achieve the best capture performance could be a research topic.
\section{Related Work}
\label{sec:related}
Many research projects have been done in exploring the indoor localization using Bluetooth signals. In \cite{cheung2006inexpensive}, Cheung et al. describes an indoor positioning system with Bluetooth beacons for a low cost (\$20). The approach is to place a beacon with a unique signature inside every room. So the smartphone can identify the signature to tell which room it is in. Their system achieves room-level precision. \cite{kajioka2014experiment} talks about an experiment of evaluating the accuracy of indoor positioning using BLE. This work uses 22 BLE beacons inside and outside of a 10.5m x 15.6m classroom. The localization granularity is less than 1m. They divide the space in the room and outside of the room into 56 regions. And generate 5800 training examples for later position identifying. In determining the user's location in experiment, the current RSSI signal is matched against pre-generated data using template matching based on Sum of Squared Difference (SSD). The accuracy is as high as 95\%. \cite{inoue2009indoor} describes a system similar to \cite{kajioka2014experiment}, but the system estimates a state transition using movement model. It is then followed by a probability distribution calculation to better determine the location. \cite{faragher2014analysis,faragher2015location} are analysis of the accuracy of BLE indoor positioning. It discusses about the fast fading effects in BLE indoor positioning and its impact on the positioning accuracy, it also mentions that the existence of the WiFi signal in the space also impacts the positioning accuracy. The solution they proposed is to migrate a batch of RSSI readings (multipath mitigation) by taking the median of a batch of values. They showed that this improves the RSSI fingerprinting performance. They also further proposed the optimal BLE deployment density and argue that more dense beacon deployment does not improve the system performance after a certain threshold is reached.

Multiple work have been done in using one or multiple sensor inputs in mobile devices for augmented reality applications, including~\cite{ababsa2009advanced, al2013indoor, deniz2014vision, guan2016vision, dissanayake2001solution, aulinas2008slam}. There is a category of technologies named SLAM that explores the environment and locates the viewer simultaneously~\cite{dissanayake2001solution, aulinas2008slam}, which can be considered as an alternative solution to our localization and pose estimation problem. \cite{deniz2014vision, guan2016vision} only uses visual information in determine the viewer's pose and location. Instead of using ORB features, \cite{guan2016vision} used SURF feature for image matching. In~\cite{ababsa2009advanced} the author combined multiple sensor inputs using Kalman filter, but their work is to build an outdoor localization system, where GPS signal is used as one sensor input.
